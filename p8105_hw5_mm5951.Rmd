---
title: "Homework 5"
author: "mm5951"
date: "`r Sys.Date()`"
output: github_document
---

```{r, include = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(rvest)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Solutions provided by teaching team.


## Problem 2

### Data wrangling & description 

First, I import the dataset using `read_csv()` and rename empty observations to "na" ("", "Unknown"). To describe the raw data, I use the `skimr::skim()` function.

```{r, warning = FALSE}
homicide_df_raw = read_csv("./data/homicide-data.csv", na = c("","Unknown"))

skimr::skim(homicide_df_raw)
```

The "homicide_df_raw" dataset contains `r ncol(homicide_df_raw)` variables and `r nrow(homicide_df_raw)` observations. Details on the nature of its variables and summary values are found in the outputs above.

Then, I wrangle data as per problem instructions. This includes:

* Create a new "city_state" variable (e.g. “Baltimore, MD”) with `mutate()` and order by alphabetical orden using `str_c()`.
* Create a new "resolution" variable using the `case_when()` syntaxis, indicating whehter a case is resolved or not (those for which the disposition is “Closed without arrest” or “Open/No arrest”).
* Note one entry "Tulsa, AL" is excluded using `filter()`, as it is unclear whether it refers to Tulsa, Oklahoma or Birmingham, Alabama (this entry becomes apparent on the next section, under the "US_summary" data frame, and is then retroactively amended).

```{r}
homicide_df = homicide_df_raw %>% 
  mutate(city_state = str_c(city, state, sep = ", "),
         resolution = case_when(
           disposition == "Closed without arrest" ~ "unsolved",
           disposition == "Open/No arrest" ~ "unsolved",
           disposition == "Closed by arrest" ~ "solved"
         )) %>% 
  relocate(city_state) %>% 
  filter(city_state != "Tulsa, AL")
```

Then, I summarize within cities using `group_by()` to obtain the total number of homicides ("n") and the number of unsolved homicides ("unsolved") in a table using `knitr::kable()`. 

```{r}
US_summary = 
  homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    n = n(),
    unsolved = sum(resolution == "unsolved"),
      ) %>% 
  knitr::kable()

US_summary
```


### Baltimore city estimates 

Next, I use the `prop.test` function to perform a test of equal proportions *(that is, testing the null that the proportions (probabilities of success) in several groups are the same, or that they equal certain given values.)*

More specifically, for the city of Baltimore, MD, I estimate the **proportion of homicides that are unsolved**. To do so, first a summary dataframe is generated containing the sum of "unsolved" crimes and the overall sample size ("n"). Then, 
A "baltimore_test" R object (a list) is generated as an output of the `prop.test`, to which then I apply `broom::tidy`. It contains the estimated proportion and confidence intervals from the test of equal proportions.

```{r}
baltimore_df = homicide_df %>% 
  filter(city_state == "Baltimore, MD")

baltimore_summary = 
  baltimore_df %>% 
  summarize(
    unsolved = sum(resolution == "unsolved"),
    n = n()
  )

baltimore_test = prop.test(
  x = baltimore_summary %>% pull(unsolved), 
  n = baltimore_summary %>% pull(n)) 

baltimore_test %>% 
  broom::tidy()
```

As a result of this test, Baltimore, MD has a proportion of 0.646 (0.628, 0.663) unresolved crimes.


### Iteration on multiple cities estimates 

Next, I run the `prop.test()` for each of the cities in the "homicide_df" dataset. To extract both the proportion of unsolved homicides and the confidence interval for each of the cities, I follow a “tidy” pipeline which entails:.

1. **Writing a function that runs the `prop.test()` in each of the cities**. The `prop_test_unresolved` uses the "US_summary" created above (without the `knitr::kable()` chunk) and a similar structure to the equal proportions test applied to Baltimore, MD.

```{r}
prop_test_unresolved = function(homicide_df) {
  
  cities_summary = 
  homicide_df %>% 
  summarize(
    n = n(),
    unsolved = sum(resolution == "unsolved"),
      )
  
  city_test = prop.test(
    x = cities_summary %>% pull(unsolved), 
    n = cities_summary %>% pull(n))
  
  return(city_test)
}
```

2. **Iterating across all cities**. To do so, the data is first nested, then `purrr::map()` is applied indicating the list ("data") I iterate over, and the function to apply (`prop_test_homicide()`. Then, a "tidy_results" object is generated with the results, which is then unnested to select the variables of interest ("city_state", "estimate", and confidence intervals (CI)). Results are visualized below using `knitr::kable()`.

```{r}
results_df = 
  homicide_df %>% 
  nest(data = uid:resolution) %>% 
  mutate(
    test_results = map(data, prop_test_unresolved),
    tidy_results = map(test_results, broom::tidy)
  ) %>% 
  select(city_state, tidy_results) %>% 
  unnest(tidy_results) %>% 
  select(city_state, estimate, starts_with("conf"))

results_df %>% 
  knitr::kable(digits = 3)
```


### Data visualization

Finally, results as above are summarised in a plot that shows the estimates and CIs for each city. In doing so:

* Cities are reordered by estimate of unresolved crimes using `fct_reorder()`;
* Data is visualized using `ggplot()` and the scatterplot `geom_point()`;
* CIs are visualized using `geom_errorbar()`

```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  labs(
    title = "Estimated proportion of unresolved homicides and CI",
    x = "City, State",
    y = "Estimate")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
        plot.title = element_text(hjust = 0.5))
```



## Problem 3

**PROMPT:** When designing an experiment or analysis, a common question is whether it is likely that a true effect will be detected – put differently, whether a false null hypothesis will be rejected. The probability that a false null hypothesis is rejected is referred to as power, and it depends on several factors, including: the sample size; the effect size; and the error variance. In this problem, you will conduct a simulation to explore power in a one-sample t-test.

### Dataset generation

First, I generate a "raw dataset" with the indicated sample size (n=30), mean (μ=0) and standard deviation (σ=5):

```{r}
raw_dataset = rnorm(30, 0, 5) %>% 
  as_tibble()
```

Next, through repeated sampling I generate 5,000 datasets. In order to do so, I will

(1) generate a function `norm_dataset()` that creates a dataset following a normal distribution x∼Normal[μ,σ]
(2) generate a "norm_results_df" with fixed sample size of n=30 and 5,000 iterations

```{r}
norm_dataset = function(x) {
  rnorm(n, x, sd)}

norm_results_df = 
  expand_grid(
    sample_size = 30,
    iteration = 1:50
  ) %>% 
  mutate(
    estimate_df = map(sample_size, norm_dataset)
  ) %>% 
  unnest(estimate_df)


## Alternative, fixing n, leaving mu free

norm_dataset = function(n = 30, mu, sigma = 5) {
  
  norm_dataset = tibble(
    x = rnorm(n, x, sd),
  )
  
  norm_dataset %>% 
    summarize(
      mu_hat = mean(x),
      sigma_hat = sd(x)
    )
}

norm_results_df = 
  expand_grid(
    x = 0,
    iteration = 1:50
    ) %>% 
  mutate(
    estimate_df = map(x, norm_dataset)
  ) %>% 
  unnest(estimate_df)

```




```{r}

## Niklas code

n = 30
sd = 5
true_mean = 0:6

dataset = function(x) {
  rnorm(n, x, sd)}

output = vector("list", length = 50)
inter <- data.frame(matrix(ncol = 2, nrow = 50))
colnames(inter) <- c('true_mean', 'p-value')

combine_function = function(y){
  inter2 = list()
  for (i in 1:50) {
    output[[i]] = dataset(y)
    a = broom::tidy(t.test(output[[i]]))
    inter[i,1] = a[[1]]
    inter[i,2] = a[[3]]}
  inter2 = rbind(inter2, inter)}

final = map(true_mean, combine_function)
```

For each dataset, save μ̂and the p-value arising from a test of H:μ=0 using α=0.05
Hint: to obtain the estimate and p-value, use broom::tidy to clean the output of t.test.

Repeat the above for μ={1,2,3,4,5,6}, and complete the following:

### Data visualization

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. Describe the association between effect size and power.

Make a plot showing the average estimate of μ̂
 on the y axis and the true value of μ
 on the x axis. 
 
Make a second plot (or overlay on the first) the average estimate of μ̂
 only in samples for which the null was rejected on the y axis and the true value of μ
 on the x axis. 
 
Is the sample average of μ̂ across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?


-------


```{r}


```
